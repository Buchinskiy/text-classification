{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model wrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#keras to prepare data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#laeyers\n",
    "from keras.layers import Embedding, Input, Conv1D, MaxPooling1D, Flatten, Dense, SpatialDropout1D, Dropout\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "#models\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#for custom metric function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "\n",
    "#optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#wrapper\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading data and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "raw_data = pd.read_csv('../data/class_data_001.csv', header = None)\n",
    "raw_data.columns = ['label', 'text']\n",
    "texts = raw_data.text.tolist()\n",
    "labels = raw_data.label.values\n",
    "\n",
    "#First of all we need to tokenize text. i'am using keras tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#lets compute max sequence lenth. We have to know it because it is the size of input vector.\n",
    "#All text with lenth less then maximum should be padded to this size.\n",
    "MAX_SEQUENCE_LENGTH = max(map(len, sequences))\n",
    "\n",
    "#lets add 0 at the begining of sequenses (perform padding).\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../data/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    #As i mention above 1st element is the token. We keep token in string word\n",
    "    word = values[0]\n",
    "    #The other elements is the elements of real value vector. We keep it as np.array.\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    #Collecting dict. The keys of this dict is tokens and values is real vector elemnts.\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "def creat_emb(random_init = False):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            # probably we could random initialize them\n",
    "            # to do this use random_init = True\n",
    "        elif random_init:\n",
    "            embedding_matrix[i] = np.random.rand(50)\n",
    "    return embedding_matrix\n",
    "\n",
    "emb = creat_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#defining our build_fn\n",
    "#for example i want to be able to chose how many filters to use in the model  using sklearn interface\n",
    "#and what metric to use\n",
    "def create_simple_CNN(filter_num = 64):\n",
    "    #CONSTRUCTING MODEL\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[emb],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "    simple_cnn = Sequential([\n",
    "        embedding_layer,\n",
    "        SpatialDropout1D(0.2),\n",
    "        Dropout(0.25),\n",
    "        Conv1D(filter_num, 5, padding='same', activation='relu'), #insted number i use the param \n",
    "        Dropout(0.25),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.7),\n",
    "        Dense(1, activation='sigmoid')])\n",
    "    #COMPILING MODEL\n",
    "    #there is no reason to add mcc for keras because sklearn will use his own metrics\n",
    "    simple_cnn.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['acc']) \n",
    "    #RETURNING MODEL\n",
    "    return simple_cnn\n",
    "\n",
    "#now iam gonna use class keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params) to get\n",
    "#instance of wrapped CNN\n",
    "\n",
    "#also in this step i have to add default vlues for parameters\n",
    "sklearn_model = KerasClassifier(build_fn = create_simple_CNN, epochs=5, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracy is 0.833113565259\n",
      "CPU times: user 1min 11s, sys: 11.7 s, total: 1min 23s\n",
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#cross val score will predict accuracy by default\n",
    "print( 'Cross validation accuracy is {}'.format(cross_val_score(sklearn_model, X, y, cv = 5).mean()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation mcc is [ 0.          0.          0.          0.73930848  0.        ]\n",
      "CPU times: user 1min 23s, sys: 12.5 s, total: 1min 36s\n",
      "Wall time: 58.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# class sklearn.metrics.matthews_corrcoef is not a scorer it is a metric so i have to make scorer\n",
    "skl_mcc = make_scorer(matthews_corrcoef)\n",
    "\n",
    "sklearn_model = KerasClassifier(build_fn = create_simple_CNN, epochs=5, batch_size=32, verbose=0)\n",
    "print( 'Cross validation mcc is {}'.format(cross_val_score(sklearn_model, X, y, cv = 5, scoring = skl_mcc)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation mcc is [ 0.76873419  0.72804584  0.7680874   0.6925121   0.76535267]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "print( 'Cross validation mcc is {}'.format(cross_val_score(sklearn_model, X, y, cv = kfold, scoring = skl_mcc)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation cv = INT should use StratifiedKFold with nuber of folds INT, but now i dont think so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprising low time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model.predict(X[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10878825,  0.89121175],\n",
       "       [ 0.20608097,  0.79391903],\n",
       "       [ 0.28616792,  0.71383208],\n",
       "       [ 0.16331297,  0.83668703],\n",
       "       [ 0.19768989,  0.80231011]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model.predict_proba(X[-5:,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
